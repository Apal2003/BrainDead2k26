{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14753274,"sourceType":"datasetVersion","datasetId":9429375}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-06T12:51:08.449772Z","iopub.execute_input":"2026-02-06T12:51:08.450042Z","iopub.status.idle":"2026-02-06T12:51:10.493944Z","shell.execute_reply.started":"2026-02-06T12:51:08.450012Z","shell.execute_reply":"2026-02-06T12:51:10.493177Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/movie-recommendation/movies.csv\n/kaggle/input/movie-recommendation/ratings.csv\n/kaggle/input/movie-recommendation/README.txt\n/kaggle/input/movie-recommendation/tags.csv\n/kaggle/input/movie-recommendation/links.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y numpy\n!pip install \"numpy<2\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:24:39.913992Z","iopub.execute_input":"2026-02-06T14:24:39.914422Z","iopub.status.idle":"2026-02-06T14:24:51.594023Z","shell.execute_reply.started":"2026-02-06T14:24:39.914387Z","shell.execute_reply":"2026-02-06T14:24:51.593003Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 2.0.2\nUninstalling numpy-2.0.2:\n  Successfully uninstalled numpy-2.0.2\nCollecting numpy<2\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --force-reinstall pandas scipy scikit-surprise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:45:08.205552Z","iopub.execute_input":"2026-02-06T14:45:08.205949Z","iopub.status.idle":"2026-02-06T14:47:03.972636Z","shell.execute_reply.started":"2026-02-06T14:45:08.205920Z","shell.execute_reply":"2026-02-06T14:47:03.971668Z"}},"outputs":[{"name":"stdout","text":"Collecting pandas\n  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy\n  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-surprise\n  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nCollecting numpy>=1.26.0 (from pandas)\n  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nCollecting python-dateutil>=2.8.2 (from pandas)\n  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting joblib>=1.2.0 (from scikit-surprise)\n  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nDownloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\nBuilding wheels for collected packages: scikit-surprise\n  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp312-cp312-linux_x86_64.whl size=2554977 sha256=e101ce6a8b6fd2595123dee1bbe206d508a6109d10cf46960276c62d527e49c9\n  Stored in directory: /root/.cache/pip/wheels/75/fa/bc/739bc2cb1fbaab6061854e6cfbb81a0ae52c92a502a7fa454b\nSuccessfully built scikit-surprise\nInstalling collected packages: six, numpy, joblib, scipy, python-dateutil, scikit-surprise, pandas\n  Attempting uninstall: six\n    Found existing installation: six 1.17.0\n    Uninstalling six-1.17.0:\n      Successfully uninstalled six-1.17.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.3\n    Uninstalling joblib-1.5.3:\n      Successfully uninstalled joblib-1.5.3\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n  Attempting uninstall: python-dateutil\n    Found existing installation: python-dateutil 2.9.0.post0\n    Uninstalling python-dateutil-2.9.0.post0:\n      Successfully uninstalled python-dateutil-2.9.0.post0\n  Attempting uninstall: scikit-surprise\n    Found existing installation: scikit-surprise 1.1.4\n    Uninstalling scikit-surprise-1.1.4:\n      Successfully uninstalled scikit-surprise-1.1.4\n  Attempting uninstall: pandas\n    Found existing installation: pandas 2.2.2\n    Uninstalling pandas-2.2.2:\n      Successfully uninstalled pandas-2.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\nydata-profiling 4.18.1 requires pandas!=1.4.0,<3.0,>1.5, but you have pandas 3.0.0 which is incompatible.\nydata-profiling 4.18.1 requires scipy<1.17,>=1.8, but you have scipy 1.17.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 3.0.0 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ndask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\ncupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\ncudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 3.0.0 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pandas<3.0,>=1.0, but you have pandas 3.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, but you have pandas 3.0.0 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\ntensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed joblib-1.5.3 numpy-2.4.2 pandas-3.0.0 python-dateutil-2.9.0.post0 scikit-surprise-1.1.4 scipy-1.17.0 six-1.17.0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nReelSense: Explainable Movie Recommender System\nKaggle Training Pipeline\n\"\"\"\n\n# ============================================================================\n# PART 1: IMPORTS AND SETUP\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\n\n# Surprise library for Matrix Factorization\nfrom surprise import SVD, Dataset, Reader\nfrom surprise.model_selection import cross_validate\n\n# For saving models\nimport pickle\nimport json\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"âœ… All libraries imported successfully!\")\n\n# ============================================================================\n# PART 2: DATA LOADING\n# ============================================================================\n\n# Load datasets\nratings_df = pd.read_csv('/kaggle/input/movie-recommendation/ratings.csv')\nmovies_df = pd.read_csv('/kaggle/input/movie-recommendation/movies.csv')\ntags_df = pd.read_csv('/kaggle/input/movie-recommendation/tags.csv')\nlinks_df = pd.read_csv('/kaggle/input/movie-recommendation/links.csv')\n\nprint(\"ðŸ“Š Dataset Statistics:\")\nprint(f\"Ratings: {len(ratings_df):,} rows\")\nprint(f\"Movies: {len(movies_df):,} rows\")\nprint(f\"Tags: {len(tags_df):,} rows\")\nprint(f\"Links: {len(links_df):,} rows\")\nprint(f\"\\nUsers: {ratings_df['userId'].nunique():,}\")\nprint(f\"Movies rated: {ratings_df['movieId'].nunique():,}\")\n\n# Display samples\nprint(\"\\nðŸ“‹ Sample Data:\")\nprint(\"\\nRatings:\")\nprint(ratings_df.head())\nprint(\"\\nMovies:\")\nprint(movies_df.head())\nprint(\"\\nTags:\")\nprint(tags_df.head())\n\n# ============================================================================\n# PART 3: DATA PREPROCESSING\n# ============================================================================\n\nprint(\"\\nðŸ§¹ PREPROCESSING DATA...\")\n\n# Convert timestamp to datetime\nratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], unit='s')\nif 'timestamp' in tags_df.columns:\n    tags_df['timestamp'] = pd.to_datetime(tags_df['timestamp'], unit='s')\n\n# Sort by timestamp\nratings_df = ratings_df.sort_values('timestamp')\n\n# Clean genres\nmovies_df['genres_list'] = movies_df['genres'].str.split('|')\nmovies_df['genres_clean'] = movies_df['genres'].str.replace('|', ' ')\n\n# Clean tags\nif len(tags_df) > 0:\n    tags_df['tag'] = tags_df['tag'].str.lower().str.strip()\n    \n    # Aggregate tags per movie\n    movie_tags = tags_df.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n    movie_tags.columns = ['movieId', 'tags_combined']\n    \n    # Merge with movies\n    movies_df = movies_df.merge(movie_tags, on='movieId', how='left')\n    movies_df['tags_combined'] = movies_df['tags_combined'].fillna('')\nelse:\n    movies_df['tags_combined'] = ''\n\n# Create content features (genres + tags)\nmovies_df['content_features'] = movies_df['genres_clean'] + ' ' + movies_df['tags_combined']\n\n# Extract year from title\nmovies_df['year'] = movies_df['title'].str.extract(r'\\((\\d{4})\\)').astype(float)\n\nprint(\"âœ… Preprocessing complete!\")\n\n# ============================================================================\n# PART 4: EXPLORATORY DATA ANALYSIS\n# ============================================================================\n\nprint(\"\\nðŸ“Š PERFORMING EDA...\")\n\n# Create EDA directory\nimport os\nos.makedirs('eda_plots', exist_ok=True)\n\n# 1. Rating Distribution\nplt.figure(figsize=(10, 6))\nratings_df['rating'].value_counts().sort_index().plot(kind='bar', color='steelblue')\nplt.title('Distribution of Ratings', fontsize=16, fontweight='bold')\nplt.xlabel('Rating', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.savefig('eda_plots/rating_distribution.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 2. User Activity Distribution\nplt.figure(figsize=(12, 6))\nuser_activity = ratings_df['userId'].value_counts()\nplt.hist(user_activity, bins=50, color='coral', edgecolor='black')\nplt.title('User Activity Distribution', fontsize=16, fontweight='bold')\nplt.xlabel('Number of Ratings per User', fontsize=12)\nplt.ylabel('Number of Users', fontsize=12)\nplt.axvline(user_activity.mean(), color='red', linestyle='--', label=f'Mean: {user_activity.mean():.1f}')\nplt.legend()\nplt.tight_layout()\nplt.savefig('eda_plots/user_activity.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 3. Movie Popularity (Long-tail)\nplt.figure(figsize=(12, 6))\nmovie_counts = ratings_df['movieId'].value_counts()\nplt.plot(range(len(movie_counts)), sorted(movie_counts.values, reverse=True), color='darkgreen')\nplt.title('Long-tail Distribution of Movie Popularity', fontsize=16, fontweight='bold')\nplt.xlabel('Movie Rank', fontsize=12)\nplt.ylabel('Number of Ratings', fontsize=12)\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('eda_plots/movie_longtail.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 4. Genre Popularity\nall_genres = []\nfor genres_list in movies_df['genres_list']:\n    if isinstance(genres_list, list):\n        all_genres.extend(genres_list)\n\ngenre_counts = pd.Series(all_genres).value_counts().head(15)\n\nplt.figure(figsize=(12, 6))\ngenre_counts.plot(kind='barh', color='purple')\nplt.title('Top 15 Most Popular Genres', fontsize=16, fontweight='bold')\nplt.xlabel('Count', fontsize=12)\nplt.ylabel('Genre', fontsize=12)\nplt.tight_layout()\nplt.savefig('eda_plots/genre_popularity.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 5. Average Rating by Genre\ngenre_ratings = {}\nfor idx, row in movies_df.iterrows():\n    if isinstance(row['genres_list'], list):\n        movie_ratings = ratings_df[ratings_df['movieId'] == row['movieId']]['rating']\n        if len(movie_ratings) > 0:\n            avg_rating = movie_ratings.mean()\n            for genre in row['genres_list']:\n                if genre not in genre_ratings:\n                    genre_ratings[genre] = []\n                genre_ratings[genre].append(avg_rating)\n\ngenre_avg = {k: np.mean(v) for k, v in genre_ratings.items() if len(v) > 10}\ngenre_avg_series = pd.Series(genre_avg).sort_values(ascending=False)\n\nplt.figure(figsize=(12, 6))\ngenre_avg_series.plot(kind='barh', color='teal')\nplt.title('Average Rating by Genre', fontsize=16, fontweight='bold')\nplt.xlabel('Average Rating', fontsize=12)\nplt.ylabel('Genre', fontsize=12)\nplt.tight_layout()\nplt.savefig('eda_plots/genre_avg_rating.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 6. Rating Trends Over Time\nratings_df['year_rated'] = ratings_df['timestamp'].dt.year\nyearly_stats = ratings_df.groupby('year_rated').agg({\n    'rating': ['mean', 'count']\n}).reset_index()\nyearly_stats.columns = ['year', 'avg_rating', 'count']\n\nfig, ax1 = plt.subplots(figsize=(12, 6))\nax2 = ax1.twinx()\n\nax1.plot(yearly_stats['year'], yearly_stats['avg_rating'], 'b-o', label='Avg Rating')\nax2.bar(yearly_stats['year'], yearly_stats['count'], alpha=0.3, color='orange', label='Count')\n\nax1.set_xlabel('Year', fontsize=12)\nax1.set_ylabel('Average Rating', fontsize=12, color='b')\nax2.set_ylabel('Number of Ratings', fontsize=12, color='orange')\nax1.set_title('Rating Trends Over Time', fontsize=16, fontweight='bold')\nax1.tick_params(axis='y', labelcolor='b')\nax2.tick_params(axis='y', labelcolor='orange')\nplt.tight_layout()\nplt.savefig('eda_plots/rating_trends.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(\"âœ… EDA plots saved to 'eda_plots/' directory\")\n\n# ============================================================================\n# PART 5: TRAIN-TEST SPLIT (LEAVE-LAST-N)\n# ============================================================================\n\nprint(\"\\nðŸ”€ CREATING TRAIN-TEST SPLIT...\")\n\n# Leave-last-1 per user\ndef leave_last_n_split(df, n=1):\n    \"\"\"Split data leaving last N ratings per user for testing\"\"\"\n    train_list = []\n    test_list = []\n    \n    for user_id in df['userId'].unique():\n        user_data = df[df['userId'] == user_id].sort_values('timestamp')\n        \n        if len(user_data) > n:\n            train_list.append(user_data.iloc[:-n])\n            test_list.append(user_data.iloc[-n:])\n        else:\n            train_list.append(user_data)\n    \n    train_df = pd.concat(train_list, ignore_index=True)\n    test_df = pd.concat(test_list, ignore_index=True)\n    \n    return train_df, test_df\n\ntrain_ratings, test_ratings = leave_last_n_split(ratings_df, n=1)\n\nprint(f\"Train set: {len(train_ratings):,} ratings\")\nprint(f\"Test set: {len(test_ratings):,} ratings\")\nprint(f\"Train users: {train_ratings['userId'].nunique()}\")\nprint(f\"Test users: {test_ratings['userId'].nunique()}\")\n\n# ============================================================================\n# PART 6: MODEL 1 - POPULARITY-BASED RECOMMENDER\n# ============================================================================\n\nprint(\"\\nðŸŒŸ BUILDING POPULARITY-BASED MODEL...\")\n\nclass PopularityRecommender:\n    def __init__(self):\n        self.popular_movies = None\n        self.movie_data = None\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train popularity model\"\"\"\n        movie_stats = ratings_df.groupby('movieId').agg({\n            'rating': ['count', 'mean']\n        }).reset_index()\n        movie_stats.columns = ['movieId', 'rating_count', 'rating_mean']\n        \n        # Filter movies with at least 50 ratings\n        movie_stats = movie_stats[movie_stats['rating_count'] >= 50]\n        \n        # Calculate popularity score (weighted rating)\n        C = movie_stats['rating_mean'].mean()\n        m = movie_stats['rating_count'].quantile(0.75)\n        \n        movie_stats['popularity_score'] = (\n            (movie_stats['rating_count'] / (movie_stats['rating_count'] + m)) * movie_stats['rating_mean'] +\n            (m / (movie_stats['rating_count'] + m)) * C\n        )\n        \n        # Merge with movie data\n        self.popular_movies = movie_stats.merge(movies_df, on='movieId')\n        self.popular_movies = self.popular_movies.sort_values('popularity_score', ascending=False)\n        self.movie_data = movies_df\n        \n        print(f\"âœ… Trained on {len(self.popular_movies)} popular movies\")\n        return self\n    \n    def recommend(self, user_id=None, n=10):\n        \"\"\"Get top N popular movies\"\"\"\n        recommendations = self.popular_movies.head(n)[['movieId', 'title', 'genres', 'popularity_score']]\n        return recommendations.to_dict('records')\n    \n    def explain(self, movie_id):\n        \"\"\"Generate explanation for recommendation\"\"\"\n        movie = self.popular_movies[self.popular_movies['movieId'] == movie_id]\n        if len(movie) > 0:\n            movie = movie.iloc[0]\n            return f\"This movie is highly popular with {int(movie['rating_count'])} ratings and an average score of {movie['rating_mean']:.2f}/5.0\"\n        return \"Popular movie among users\"\n\n# Train and save\npop_model = PopularityRecommender()\npop_model.fit(train_ratings, movies_df)\n\nwith open('models/popularity_model.pkl', 'wb') as f:\n    pickle.dump(pop_model, f)\n\nprint(\"âœ… Popularity model saved!\")\n\n# ============================================================================\n# PART 7: MODEL 2 - COLLABORATIVE FILTERING (USER-USER)\n# ============================================================================\n\nprint(\"\\nðŸ‘¥ BUILDING USER-USER COLLABORATIVE FILTERING...\")\n\nclass UserUserCF:\n    def __init__(self, k=20):\n        self.k = k  # Number of neighbors\n        self.user_item_matrix = None\n        self.user_similarity = None\n        self.movies_df = None\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train user-user CF model\"\"\"\n        # Create user-item matrix\n        self.user_item_matrix = ratings_df.pivot(\n            index='userId', \n            columns='movieId', \n            values='rating'\n        ).fillna(0)\n        \n        # Calculate user similarity (cosine)\n        print(\"Calculating user similarity matrix...\")\n        user_matrix = self.user_item_matrix.values\n        self.user_similarity = cosine_similarity(user_matrix)\n        \n        self.movies_df = movies_df\n        print(f\"âœ… Trained on {len(self.user_item_matrix)} users\")\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N recommendations for user\"\"\"\n        if user_id not in self.user_item_matrix.index:\n            return []\n        \n        user_idx = self.user_item_matrix.index.get_loc(user_id)\n        \n        # Get similar users\n        user_similarities = self.user_similarity[user_idx]\n        similar_user_indices = user_similarities.argsort()[::-1][1:self.k+1]\n        \n        # Get movies rated by similar users\n        user_ratings = self.user_item_matrix.iloc[user_idx]\n        unrated_movies = user_ratings[user_ratings == 0].index\n        \n        # Predict ratings for unrated movies\n        predictions = {}\n        for movie_id in unrated_movies:\n            if movie_id in self.user_item_matrix.columns:\n                movie_ratings = self.user_item_matrix[movie_id].iloc[similar_user_indices]\n                similarities = user_similarities[similar_user_indices]\n                \n                # Weighted average\n                rated_mask = movie_ratings > 0\n                if rated_mask.sum() > 0:\n                    pred = np.sum(movie_ratings[rated_mask] * similarities[rated_mask]) / np.sum(similarities[rated_mask])\n                    predictions[movie_id] = pred\n        \n        # Get top N\n        top_movies = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:n]\n        \n        # Format recommendations\n        recommendations = []\n        for movie_id, score in top_movies:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'predicted_rating': score\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation\"\"\"\n        return f\"Recommended because users similar to you enjoyed this movie\"\n\n# Train and save\nuser_cf_model = UserUserCF(k=20)\nuser_cf_model.fit(train_ratings, movies_df)\n\nwith open('models/user_cf_model.pkl', 'wb') as f:\n    pickle.dump(user_cf_model, f)\n\nprint(\"âœ… User-User CF model saved!\")\n\n# ============================================================================\n# PART 8: MODEL 3 - ITEM-ITEM COLLABORATIVE FILTERING\n# ============================================================================\n\nprint(\"\\nðŸŽ¬ BUILDING ITEM-ITEM COLLABORATIVE FILTERING...\")\n\nclass ItemItemCF:\n    def __init__(self, k=20):\n        self.k = k\n        self.item_similarity = None\n        self.user_ratings = None\n        self.movies_df = None\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train item-item CF model\"\"\"\n        # Create item-user matrix\n        item_user_matrix = ratings_df.pivot(\n            index='movieId',\n            columns='userId',\n            values='rating'\n        ).fillna(0)\n        \n        # Calculate item similarity\n        print(\"Calculating item similarity matrix...\")\n        self.item_similarity = cosine_similarity(item_user_matrix.values)\n        self.item_similarity = pd.DataFrame(\n            self.item_similarity,\n            index=item_user_matrix.index,\n            columns=item_user_matrix.index\n        )\n        \n        # Store user ratings for predictions\n        self.user_ratings = ratings_df.groupby('userId').apply(\n            lambda x: dict(zip(x['movieId'], x['rating']))\n        ).to_dict()\n        \n        self.movies_df = movies_df\n        print(f\"âœ… Trained on {len(item_user_matrix)} items\")\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N recommendations\"\"\"\n        if user_id not in self.user_ratings:\n            return []\n        \n        user_rated_movies = self.user_ratings[user_id]\n        \n        # Find similar items to user's rated items\n        predictions = {}\n        for rated_movie, rating in user_rated_movies.items():\n            if rated_movie in self.item_similarity.index:\n                similar_movies = self.item_similarity[rated_movie]\n                \n                for movie_id, similarity in similar_movies.items():\n                    if movie_id not in user_rated_movies:\n                        if movie_id not in predictions:\n                            predictions[movie_id] = 0\n                        predictions[movie_id] += similarity * rating\n        \n        # Normalize and get top N\n        top_movies = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:n]\n        \n        recommendations = []\n        for movie_id, score in top_movies:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'predicted_rating': score\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation\"\"\"\n        if user_id in self.user_ratings:\n            user_movies = list(self.user_ratings[user_id].keys())\n            if movie_id in self.item_similarity.index:\n                similar = self.item_similarity[movie_id][user_movies].nlargest(3)\n                similar_titles = self.movies_df[self.movies_df['movieId'].isin(similar.index)]['title'].tolist()\n                if similar_titles:\n                    return f\"Similar to movies you liked: {', '.join(similar_titles[:2])}\"\n        return \"Based on your viewing history\"\n\n# Train and save\nitem_cf_model = ItemItemCF(k=20)\nitem_cf_model.fit(train_ratings, movies_df)\n\nwith open('models/item_cf_model.pkl', 'wb') as f:\n    pickle.dump(item_cf_model, f)\n\nprint(\"âœ… Item-Item CF model saved!\")\n\n# ============================================================================\n# PART 9: MODEL 4 - MATRIX FACTORIZATION (SVD)\n# ============================================================================\n\nprint(\"\\nðŸ§® BUILDING MATRIX FACTORIZATION MODEL (SVD)...\")\n\n# Prepare data for Surprise library\nreader = Reader(rating_scale=(0.5, 5.0))\ndata = Dataset.load_from_df(train_ratings[['userId', 'movieId', 'rating']], reader)\n\n# Train SVD\nprint(\"Training SVD model...\")\nsvd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, verbose=True)\ntrainset = data.build_full_trainset()\nsvd_model.fit(trainset)\n\n# Save model\nwith open('models/svd_model.pkl', 'wb') as f:\n    pickle.dump(svd_model, f)\n\nprint(\"âœ… SVD model saved!\")\n\n# Evaluation on train set\nfrom surprise import accuracy\n\npredictions = svd_model.test(trainset.build_testset())\nprint(f\"\\nTrain RMSE: {accuracy.rmse(predictions):.4f}\")\nprint(f\"Train MAE: {accuracy.mae(predictions):.4f}\")\n\n# ============================================================================\n# PART 10: MODEL 5 - CONTENT-BASED FILTERING\n# ============================================================================\n\nprint(\"\\nðŸ“ BUILDING CONTENT-BASED MODEL...\")\n\nclass ContentBasedRecommender:\n    def __init__(self):\n        self.tfidf_matrix = None\n        self.movie_indices = None\n        self.movies_df = None\n        self.user_profiles = {}\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train content-based model\"\"\"\n        # Create TF-IDF matrix\n        tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n        self.tfidf_matrix = tfidf.fit_transform(movies_df['content_features'])\n        self.movie_indices = movies_df['movieId'].values\n        self.movies_df = movies_df\n        \n        # Build user profiles based on rated movies\n        print(\"Building user profiles...\")\n        for user_id in ratings_df['userId'].unique():\n            user_ratings = ratings_df[ratings_df['userId'] == user_id]\n            \n            # Get movies rated highly (>= 4.0)\n            liked_movies = user_ratings[user_ratings['rating'] >= 4.0]['movieId'].values\n            \n            # Build profile as average of liked movie features\n            if len(liked_movies) > 0:\n                liked_indices = [np.where(self.movie_indices == mid)[0][0] \n                               for mid in liked_movies if mid in self.movie_indices]\n                \n                if liked_indices:\n                    user_profile = np.array(self.tfidf_matrix[liked_indices].mean(axis=0)).flatten()\n                    self.user_profiles[user_id] = user_profile\n        \n        print(f\"âœ… Created profiles for {len(self.user_profiles)} users\")\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N content-based recommendations\"\"\"\n        if user_id not in self.user_profiles:\n            return []\n        \n        user_profile = self.user_profiles[user_id]\n        \n        # Calculate similarity to all movies\n        similarities = cosine_similarity([user_profile], self.tfidf_matrix)[0]\n        \n        # Get top similar movies\n        top_indices = similarities.argsort()[::-1][:n*2]  # Get more for filtering\n        \n        recommendations = []\n        for idx in top_indices:\n            if len(recommendations) >= n:\n                break\n            \n            movie_id = self.movie_indices[idx]\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id].iloc[0]\n            \n            recommendations.append({\n                'movieId': movie_id,\n                'title': movie_info['title'],\n                'genres': movie_info['genres'],\n                'similarity_score': similarities[idx]\n            })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation\"\"\"\n        movie = self.movies_df[self.movies_df['movieId'] == movie_id]\n        if len(movie) > 0:\n            genres = movie.iloc[0]['genres']\n            return f\"Matches your interest in genres: {genres}\"\n        return \"Based on content similarity to your preferences\"\n\n# Train and save\ncontent_model = ContentBasedRecommender()\ncontent_model.fit(train_ratings, movies_df)\n\nwith open('models/content_model.pkl', 'wb') as f:\n    pickle.dump(content_model, f)\n\nprint(\"âœ… Content-based model saved!\")\n\n# ============================================================================\n# PART 11: HYBRID RECOMMENDER\n# ============================================================================\n\nprint(\"\\nðŸŽ¯ BUILDING HYBRID RECOMMENDER...\")\n\nclass HybridRecommender:\n    def __init__(self, models, weights=None):\n        \"\"\"\n        models: dict of {name: model}\n        weights: dict of {name: weight} (defaults to equal weights)\n        \"\"\"\n        self.models = models\n        self.weights = weights or {name: 1.0/len(models) for name in models}\n        self.movies_df = None\n    \n    def fit(self, movies_df):\n        self.movies_df = movies_df\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get hybrid recommendations\"\"\"\n        all_recommendations = {}\n        \n        # Get recommendations from each model\n        for name, model in self.models.items():\n            try:\n                recs = model.recommend(user_id, n=n*2)\n                weight = self.weights[name]\n                \n                for rec in recs:\n                    movie_id = rec['movieId']\n                    score = rec.get('predicted_rating', rec.get('popularity_score', rec.get('similarity_score', 1.0)))\n                    \n                    if movie_id not in all_recommendations:\n                        all_recommendations[movie_id] = {\n                            'score': 0,\n                            'count': 0,\n                            'sources': []\n                        }\n                    \n                    all_recommendations[movie_id]['score'] += score * weight\n                    all_recommendations[movie_id]['count'] += 1\n                    all_recommendations[movie_id]['sources'].append(name)\n            except Exception as e:\n                print(f\"Warning: {name} model failed: {e}\")\n                continue\n        \n        # Normalize scores and get top N\n        for movie_id in all_recommendations:\n            all_recommendations[movie_id]['final_score'] = (\n                all_recommendations[movie_id]['score'] / all_recommendations[movie_id]['count']\n            )\n        \n        top_movies = sorted(\n            all_recommendations.items(),\n            key=lambda x: x[1]['final_score'],\n            reverse=True\n        )[:n]\n        \n        # Format recommendations\n        recommendations = []\n        for movie_id, data in top_movies:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'hybrid_score': data['final_score'],\n                    'sources': data['sources']\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id, sources):\n        \"\"\"Generate hybrid explanation\"\"\"\n        explanations = []\n        for source in sources:\n            if source in self.models:\n                exp = self.models[source].explain(user_id, movie_id)\n                explanations.append(f\"{source}: {exp}\")\n        \n        return \" | \".join(explanations)\n\n# Create hybrid model\nhybrid_models = {\n    'user_cf': user_cf_model,\n    'item_cf': item_cf_model,\n    'content': content_model,\n}\n\nhybrid_weights = {\n    'user_cf': 0.4,\n    'item_cf': 0.4,\n    'content': 0.2\n}\n\nhybrid_model = HybridRecommender(hybrid_models, hybrid_weights)\nhybrid_model.fit(movies_df)\n\nwith open('models/hybrid_model.pkl', 'wb') as f:\n    pickle.dump(hybrid_model, f)\n\nprint(\"âœ… Hybrid model saved!\")\n\n# ============================================================================\n# PART 12: EVALUATION METRICS\n# ============================================================================\n\nprint(\"\\nðŸ“ˆ CALCULATING EVALUATION METRICS...\")\n\ndef precision_at_k(recommended, relevant, k=10):\n    \"\"\"Precision@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = len([r for r in recommended_k if r in relevant_set])\n    return hits / k if k > 0 else 0\n\ndef recall_at_k(recommended, relevant, k=10):\n    \"\"\"Recall@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = len([r for r in recommended_k if r in relevant_set])\n    return hits / len(relevant_set) if len(relevant_set) > 0 else 0\n\ndef ndcg_at_k(recommended, relevant, k=10):\n    \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    \n    dcg = sum([1.0 / np.log2(idx + 2) if rec in relevant_set else 0 \n               for idx, rec in enumerate(recommended_k)])\n    \n    idcg = sum([1.0 / np.log2(idx + 2) for idx in range(min(len(relevant), k))])\n    \n    return dcg / idcg if idcg > 0 else 0\n\ndef evaluate_model(model, test_ratings, k=10):\n    \"\"\"Evaluate a recommendation model\"\"\"\n    precisions = []\n    recalls = []\n    ndcgs = []\n    \n    # Sample users for faster evaluation\n    test_users = test_ratings['userId'].unique()[:100]  # Sample 100 users\n    \n    for user_id in test_users:\n        # Get user's test items (relevant items)\n        relevant_items = test_ratings[test_ratings['userId'] == user_id]['movieId'].tolist()\n        \n        if len(relevant_items) == 0:\n            continue\n        \n        try:\n            # Get recommendations\n            recs = model.recommend(user_id, n=k)\n            recommended_items = [r['movieId'] for r in recs]\n            \n            if len(recommended_items) > 0:\n                precisions.append(precision_at_k(recommended_items, relevant_items, k))\n                recalls.append(recall_at_k(recommended_items, relevant_items, k))\n                ndcgs.append(ndcg_at_k(recommended_items, relevant_items, k))\n        except:\n            continue\n    \n    return {\n        'Precision@K': np.mean(precisions) if precisions else 0,\n        'Recall@K': np.mean(recalls) if recalls else 0,\n        'NDCG@K': np.mean(ndcgs) if ndcgs else 0\n    }\n\n# Evaluate models\nprint(\"\\nðŸŽ¯ Evaluating models on test set...\")\n\nresults = {}\n\nprint(\"\\n1. Evaluating Hybrid Model...\")\nresults['Hybrid'] = evaluate_model(hybrid_model, test_ratings, k=10)\n\nprint(\"\\n2. Evaluating User-User CF...\")\nresults['User-User CF'] = evaluate_model(user_cf_model, test_ratings, k=10)\n\nprint(\"\\n3. Evaluating Item-Item CF...\")\nresults['Item-Item CF'] = evaluate_model(item_cf_model, test_ratings, k=10)\n\nprint(\"\\n4. Evaluating Content-Based...\")\nresults['Content-Based'] = evaluate_model(content_model, test_ratings, k=10)\n\n# Display results\nprint(\"\\n\" + \"=\"*70)\nprint(\"EVALUATION RESULTS (K=10)\")\nprint(\"=\"*70)\n\nresults_df = pd.DataFrame(results).T\nprint(results_df.to_string())\n\n# Save results\nresults_df.to_csv('evaluation_results.csv')\nprint(\"\\nâœ… Results saved to 'evaluation_results.csv'\")\n\n# ============================================================================\n# PART 13: DIVERSITY & NOVELTY METRICS\n# ============================================================================\n\nprint(\"\\nðŸŒˆ CALCULATING DIVERSITY METRICS...\")\n\ndef calculate_diversity_metrics(model, test_users, movies_df, k=10):\n    \"\"\"Calculate diversity and novelty metrics\"\"\"\n    \n    # Catalog coverage\n    all_recommended = set()\n    total_catalog = len(movies_df)\n    \n    # Intra-list diversity\n    intra_diversities = []\n    \n    for user_id in test_users[:50]:  # Sample for speed\n        try:\n            recs = model.recommend(user_id, n=k)\n            rec_ids = [r['movieId'] for r in recs]\n            \n            all_recommended.update(rec_ids)\n            \n            # Calculate genre diversity\n            rec_genres = []\n            for movie_id in rec_ids:\n                movie = movies_df[movies_df['movieId'] == movie_id]\n                if len(movie) > 0:\n                    genres = movie.iloc[0]['genres'].split('|')\n                    rec_genres.extend(genres)\n            \n            # Unique genres / total items\n            if len(rec_ids) > 0:\n                diversity = len(set(rec_genres)) / len(rec_ids)\n                intra_diversities.append(diversity)\n        except:\n            continue\n    \n    catalog_coverage = len(all_recommended) / total_catalog\n    avg_intra_diversity = np.mean(intra_diversities) if intra_diversities else 0\n    \n    return {\n        'Catalog Coverage': catalog_coverage,\n        'Intra-List Diversity': avg_intra_diversity\n    }\n\n# Calculate for hybrid model\ntest_users = test_ratings['userId'].unique()\ndiversity_metrics = calculate_diversity_metrics(hybrid_model, test_users, movies_df, k=10)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DIVERSITY METRICS (Hybrid Model)\")\nprint(\"=\"*70)\nfor metric, value in diversity_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\n# Save diversity metrics\nwith open('diversity_metrics.json', 'w') as f:\n    json.dump(diversity_metrics, f, indent=2)\n\nprint(\"\\nâœ… Diversity metrics saved to 'diversity_metrics.json'\")\n\n# ============================================================================\n# PART 14: EXPLAINABILITY EXAMPLES\n# ============================================================================\n\nprint(\"\\nðŸ’¡ GENERATING EXPLANATION EXAMPLES...\")\n\n# Get a sample user\nsample_user = train_ratings['userId'].iloc[0]\n\nprint(f\"\\nðŸ“Œ Sample Recommendations for User {sample_user}:\")\nprint(\"=\"*70)\n\n# Get hybrid recommendations\nhybrid_recs = hybrid_model.recommend(sample_user, n=5)\n\nfor i, rec in enumerate(hybrid_recs, 1):\n    print(f\"\\n{i}. {rec['title']}\")\n    print(f\"   Genres: {rec['genres']}\")\n    print(f\"   Score: {rec['hybrid_score']:.4f}\")\n    print(f\"   Sources: {', '.join(rec['sources'])}\")\n    \n    # Generate explanation\n    explanation = hybrid_model.explain(sample_user, rec['movieId'], rec['sources'])\n    print(f\"   ðŸ“– Why: {explanation}\")\n\n# ============================================================================\n# PART 15: SAVE MODEL METADATA\n# ============================================================================\n\nprint(\"\\nðŸ’¾ SAVING MODEL METADATA...\")\n\nmetadata = {\n    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'dataset_stats': {\n        'total_ratings': len(ratings_df),\n        'total_users': ratings_df['userId'].nunique(),\n        'total_movies': ratings_df['movieId'].nunique(),\n        'train_size': len(train_ratings),\n        'test_size': len(test_ratings)\n    },\n    'models': {\n        'popularity': 'PopularityRecommender',\n        'user_cf': 'UserUserCF(k=20)',\n        'item_cf': 'ItemItemCF(k=20)',\n        'svd': 'SVD(factors=100)',\n        'content': 'ContentBasedRecommender',\n        'hybrid': 'HybridRecommender(weights=user_cf:0.4, item_cf:0.4, content:0.2)'\n    },\n    'evaluation': results,\n    'diversity': diversity_metrics\n}\n\nwith open('models/model_metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"âœ… Metadata saved to 'models/model_metadata.json'\")\n\n# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŽ‰ REELSENSE TRAINING PIPELINE COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nðŸ“ Files Generated:\")\nprint(\"  âœ“ models/popularity_model.pkl\")\nprint(\"  âœ“ models/user_cf_model.pkl\")\nprint(\"  âœ“ models/item_cf_model.pkl\")\nprint(\"  âœ“ models/svd_model.pkl\")\nprint(\"  âœ“ models/content_model.pkl\")\nprint(\"  âœ“ models/hybrid_model.pkl\")\nprint(\"  âœ“ models/model_metadata.json\")\nprint(\"  âœ“ evaluation_results.csv\")\nprint(\"  âœ“ diversity_metrics.json\")\nprint(\"  âœ“ eda_plots/ (6 visualization files)\")\n\nprint(\"\\nðŸ“Š Best Model Performance:\")\nbest_model = max(results.items(), key=lambda x: x[1]['NDCG@K'])\nprint(f\"  {best_model[0]}: NDCG@10 = {best_model[1]['NDCG@K']:.4f}\")\n\nprint(\"\\nâœ¨ Next Step: Deploy with Streamlit UI!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T14:50:55.893018Z","iopub.execute_input":"2026-02-06T14:50:55.893364Z","iopub.status.idle":"2026-02-06T14:50:57.494790Z","shell.execute_reply.started":"2026-02-06T14:50:55.893338Z","shell.execute_reply":"2026-02-06T14:50:57.493857Z"}},"outputs":[{"name":"stderr","text":"\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_55/2152663439.py\", line 25, in <cell line: 0>\n    from surprise import SVD, Dataset, Reader\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\", line 6, in <module>\n    from .prediction_algorithms import (\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n    from .algo_base import AlgoBase\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n    from .. import similarities as sims\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2152663439.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Surprise library for Matrix Factorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .prediction_algorithms import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mAlgoBase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \"\"\"\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malgo_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbaseline_only\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mco_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCoClustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimilarities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbaseline_als\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPrediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPredictionImpossible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/similarities.pyx\u001b[0m in \u001b[0;36minit surprise.similarities\u001b[0;34m()\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."],"ename":"ImportError","evalue":"numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nReelSense: Explainable Movie Recommender System\nKaggle Training Pipeline - FIXED VERSION\n\"\"\"\n\n# ============================================================================\n# PART 0: FIX NUMPY COMPATIBILITY\n# ============================================================================\n\n# import subprocess\n# import sys\n\n# print(\"ðŸ”§ Checking NumPy compatibility...\")\n# try:\n#     import numpy as np\n#     if int(np.__version__.split('.')[0]) >= 2:\n#         print(\"âš ï¸ NumPy 2.x detected. Downgrading to NumPy 1.x for surprise library compatibility...\")\n#         subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"numpy<2\", \"-q\"])\n#         print(\"âœ… NumPy downgraded. Please RESTART the kernel and run again.\")\n#         print(\"   In Kaggle: Click 'Session' -> 'Restart Session'\")\n#         print(\"   In Colab: Click 'Runtime' -> 'Restart runtime'\")\n#         sys.exit(0)\n#     else:\n#         print(f\"âœ… NumPy {np.__version__} is compatible!\")\n# except:\n#     pass\n\n# ============================================================================\n# PART 1: IMPORTS AND SETUP\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Machine Learning\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom scipy.sparse import csr_matrix\n\n# Surprise library for Matrix Factorization\ntry:\n    from surprise import SVD, Dataset, Reader\n    from surprise.model_selection import cross_validate\n    SURPRISE_AVAILABLE = True\n    print(\"âœ… Surprise library loaded successfully!\")\nexcept ImportError as e:\n    print(f\"âš ï¸ Warning: Surprise library not available: {e}\")\n    print(\"   SVD model will be skipped. Other models will work fine.\")\n    SURPRISE_AVAILABLE = False\n\n# For saving models\nimport pickle\nimport json\nimport os\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 6)\n\nprint(\"âœ… All libraries imported successfully!\")\n\n# ============================================================================\n# PART 2: DATA LOADING\n# ============================================================================\n\n# Load datasets\nratings_df = pd.read_csv('/kaggle/input/movie-recommendation/ratings.csv')\nmovies_df = pd.read_csv('/kaggle/input/movie-recommendation/movies.csv')\ntags_df = pd.read_csv('/kaggle/input/movie-recommendation/tags.csv')\nlinks_df = pd.read_csv('/kaggle/input/movie-recommendation/links.csv')\n\nprint(\"ðŸ“Š Dataset Statistics:\")\nprint(f\"Ratings: {len(ratings_df):,} rows\")\nprint(f\"Movies: {len(movies_df):,} rows\")\nprint(f\"Tags: {len(tags_df):,} rows\")\nprint(f\"Links: {len(links_df):,} rows\")\nprint(f\"\\nUsers: {ratings_df['userId'].nunique():,}\")\nprint(f\"Movies rated: {ratings_df['movieId'].nunique():,}\")\n\n# Display samples\nprint(\"\\nðŸ“‹ Sample Data:\")\nprint(\"\\nRatings:\")\nprint(ratings_df.head())\nprint(\"\\nMovies:\")\nprint(movies_df.head())\nprint(\"\\nTags:\")\nprint(tags_df.head())\n\n# ============================================================================\n# PART 3: DATA PREPROCESSING\n# ============================================================================\n\nprint(\"\\nðŸ§¹ PREPROCESSING DATA...\")\n\n# Convert timestamp to datetime\nratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], unit='s')\nif 'timestamp' in tags_df.columns:\n    tags_df['timestamp'] = pd.to_datetime(tags_df['timestamp'], unit='s')\n\n# Sort by timestamp\nratings_df = ratings_df.sort_values('timestamp')\n\n# Clean genres\nmovies_df['genres_list'] = movies_df['genres'].str.split('|')\nmovies_df['genres_clean'] = movies_df['genres'].str.replace('|', ' ')\n\n# Clean tags\nif len(tags_df) > 0:\n    tags_df['tag'] = tags_df['tag'].str.lower().str.strip()\n    \n    # Aggregate tags per movie\n    movie_tags = tags_df.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n    movie_tags.columns = ['movieId', 'tags_combined']\n    \n    # Merge with movies\n    movies_df = movies_df.merge(movie_tags, on='movieId', how='left')\n    movies_df['tags_combined'] = movies_df['tags_combined'].fillna('')\nelse:\n    movies_df['tags_combined'] = ''\n\n# Create content features (genres + tags)\nmovies_df['content_features'] = movies_df['genres_clean'] + ' ' + movies_df['tags_combined']\n\n# Extract year from title\nmovies_df['year'] = movies_df['title'].str.extract(r'\\((\\d{4})\\)').astype(float)\n\nprint(\"âœ… Preprocessing complete!\")\n\n# ============================================================================\n# PART 4: EXPLORATORY DATA ANALYSIS\n# ============================================================================\n\nprint(\"\\nðŸ“Š PERFORMING EDA...\")\n\n# Create EDA directory\nos.makedirs('eda_plots', exist_ok=True)\n\n# 1. Rating Distribution\nplt.figure(figsize=(10, 6))\nratings_df['rating'].value_counts().sort_index().plot(kind='bar', color='steelblue')\nplt.title('Distribution of Ratings', fontsize=16, fontweight='bold')\nplt.xlabel('Rating', fontsize=12)\nplt.ylabel('Count', fontsize=12)\nplt.xticks(rotation=0)\nplt.tight_layout()\nplt.savefig('eda_plots/rating_distribution.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 2. User Activity Distribution\nplt.figure(figsize=(12, 6))\nuser_activity = ratings_df['userId'].value_counts()\nplt.hist(user_activity, bins=50, color='coral', edgecolor='black')\nplt.title('User Activity Distribution', fontsize=16, fontweight='bold')\nplt.xlabel('Number of Ratings per User', fontsize=12)\nplt.ylabel('Number of Users', fontsize=12)\nplt.axvline(user_activity.mean(), color='red', linestyle='--', label=f'Mean: {user_activity.mean():.1f}')\nplt.legend()\nplt.tight_layout()\nplt.savefig('eda_plots/user_activity.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 3. Movie Popularity (Long-tail)\nplt.figure(figsize=(12, 6))\nmovie_counts = ratings_df['movieId'].value_counts()\nplt.plot(range(len(movie_counts)), sorted(movie_counts.values, reverse=True), color='darkgreen')\nplt.title('Long-tail Distribution of Movie Popularity', fontsize=16, fontweight='bold')\nplt.xlabel('Movie Rank', fontsize=12)\nplt.ylabel('Number of Ratings', fontsize=12)\nplt.yscale('log')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('eda_plots/movie_longtail.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 4. Genre Popularity\nall_genres = []\nfor genres_list in movies_df['genres_list']:\n    if isinstance(genres_list, list):\n        all_genres.extend(genres_list)\n\ngenre_counts = pd.Series(all_genres).value_counts().head(15)\n\nplt.figure(figsize=(12, 6))\ngenre_counts.plot(kind='barh', color='purple')\nplt.title('Top 15 Most Popular Genres', fontsize=16, fontweight='bold')\nplt.xlabel('Count', fontsize=12)\nplt.ylabel('Genre', fontsize=12)\nplt.tight_layout()\nplt.savefig('eda_plots/genre_popularity.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 5. Average Rating by Genre\ngenre_ratings = {}\nfor idx, row in movies_df.iterrows():\n    if isinstance(row['genres_list'], list):\n        movie_ratings = ratings_df[ratings_df['movieId'] == row['movieId']]['rating']\n        if len(movie_ratings) > 0:\n            avg_rating = movie_ratings.mean()\n            for genre in row['genres_list']:\n                if genre not in genre_ratings:\n                    genre_ratings[genre] = []\n                genre_ratings[genre].append(avg_rating)\n\ngenre_avg = {k: np.mean(v) for k, v in genre_ratings.items() if len(v) > 10}\ngenre_avg_series = pd.Series(genre_avg).sort_values(ascending=False)\n\nplt.figure(figsize=(12, 6))\ngenre_avg_series.plot(kind='barh', color='teal')\nplt.title('Average Rating by Genre', fontsize=16, fontweight='bold')\nplt.xlabel('Average Rating', fontsize=12)\nplt.ylabel('Genre', fontsize=12)\nplt.tight_layout()\nplt.savefig('eda_plots/genre_avg_rating.png', dpi=300, bbox_inches='tight')\nplt.close()\n\n# 6. Rating Trends Over Time\nratings_df['year_rated'] = ratings_df['timestamp'].dt.year\nyearly_stats = ratings_df.groupby('year_rated').agg({\n    'rating': ['mean', 'count']\n}).reset_index()\nyearly_stats.columns = ['year', 'avg_rating', 'count']\n\nfig, ax1 = plt.subplots(figsize=(12, 6))\nax2 = ax1.twinx()\n\nax1.plot(yearly_stats['year'], yearly_stats['avg_rating'], 'b-o', label='Avg Rating')\nax2.bar(yearly_stats['year'], yearly_stats['count'], alpha=0.3, color='orange', label='Count')\n\nax1.set_xlabel('Year', fontsize=12)\nax1.set_ylabel('Average Rating', fontsize=12, color='b')\nax2.set_ylabel('Number of Ratings', fontsize=12, color='orange')\nax1.set_title('Rating Trends Over Time', fontsize=16, fontweight='bold')\nax1.tick_params(axis='y', labelcolor='b')\nax2.tick_params(axis='y', labelcolor='orange')\nplt.tight_layout()\nplt.savefig('eda_plots/rating_trends.png', dpi=300, bbox_inches='tight')\nplt.close()\n\nprint(\"âœ… EDA plots saved to 'eda_plots/' directory\")\n\n# ============================================================================\n# PART 5: TRAIN-TEST SPLIT (LEAVE-LAST-N)\n# ============================================================================\n\nprint(\"\\nðŸ”€ CREATING TRAIN-TEST SPLIT...\")\n\n# Leave-last-1 per user\ndef leave_last_n_split(df, n=1):\n    \"\"\"Split data leaving last N ratings per user for testing\"\"\"\n    train_list = []\n    test_list = []\n    \n    for user_id in df['userId'].unique():\n        user_data = df[df['userId'] == user_id].sort_values('timestamp')\n        \n        if len(user_data) > n:\n            train_list.append(user_data.iloc[:-n])\n            test_list.append(user_data.iloc[-n:])\n        else:\n            train_list.append(user_data)\n    \n    train_df = pd.concat(train_list, ignore_index=True)\n    test_df = pd.concat(test_list, ignore_index=True)\n    \n    return train_df, test_df\n\ntrain_ratings, test_ratings = leave_last_n_split(ratings_df, n=1)\n\nprint(f\"Train set: {len(train_ratings):,} ratings\")\nprint(f\"Test set: {len(test_ratings):,} ratings\")\nprint(f\"Train users: {train_ratings['userId'].nunique()}\")\nprint(f\"Test users: {test_ratings['userId'].nunique()}\")\n\n# Create models directory\nos.makedirs('models', exist_ok=True)\n\n# ============================================================================\n# PART 6: MODEL 1 - POPULARITY-BASED RECOMMENDER\n# ============================================================================\n\nprint(\"\\nðŸŒŸ BUILDING POPULARITY-BASED MODEL...\")\n\nclass PopularityRecommender:\n    def __init__(self):\n        self.popular_movies = None\n        self.movie_data = None\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train popularity model\"\"\"\n        movie_stats = ratings_df.groupby('movieId').agg({\n            'rating': ['count', 'mean']\n        }).reset_index()\n        movie_stats.columns = ['movieId', 'rating_count', 'rating_mean']\n        \n        # Filter movies with at least 50 ratings\n        movie_stats = movie_stats[movie_stats['rating_count'] >= 50]\n        \n        # Calculate popularity score (weighted rating)\n        C = movie_stats['rating_mean'].mean()\n        m = movie_stats['rating_count'].quantile(0.75)\n        \n        movie_stats['popularity_score'] = (\n            (movie_stats['rating_count'] / (movie_stats['rating_count'] + m)) * movie_stats['rating_mean'] +\n            (m / (movie_stats['rating_count'] + m)) * C\n        )\n        \n        # Merge with movie data\n        self.popular_movies = movie_stats.merge(movies_df, on='movieId')\n        self.popular_movies = self.popular_movies.sort_values('popularity_score', ascending=False)\n        self.movie_data = movies_df\n        \n        print(f\"âœ… Trained on {len(self.popular_movies)} popular movies\")\n        return self\n    \n    def recommend(self, user_id=None, n=10):\n        \"\"\"Get top N popular movies\"\"\"\n        recommendations = self.popular_movies.head(n)[['movieId', 'title', 'genres', 'popularity_score']]\n        return recommendations.to_dict('records')\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation for recommendation\"\"\"\n        movie = self.popular_movies[self.popular_movies['movieId'] == movie_id]\n        if len(movie) > 0:\n            movie = movie.iloc[0]\n            return f\"This movie is highly popular with {int(movie['rating_count'])} ratings and an average score of {movie['rating_mean']:.2f}/5.0\"\n        return \"Popular movie among users\"\n\n# Train and save\npop_model = PopularityRecommender()\npop_model.fit(train_ratings, movies_df)\n\nwith open('models/popularity_model.pkl', 'wb') as f:\n    pickle.dump(pop_model, f)\n\nprint(\"âœ… Popularity model saved!\")\n\n# ============================================================================\n# PART 7: MODEL 2 - COLLABORATIVE FILTERING (USER-USER)\n# ============================================================================\n\nprint(\"\\nðŸ‘¥ BUILDING USER-USER COLLABORATIVE FILTERING...\")\n\nclass UserUserCF:\n    def __init__(self, k=20):\n        self.k = k  # Number of neighbors\n        self.user_item_matrix = None\n        self.user_similarity = None\n        self.movies_df = None\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train user-user CF model\"\"\"\n        # Create user-item matrix\n        self.user_item_matrix = ratings_df.pivot(\n            index='userId', \n            columns='movieId', \n            values='rating'\n        ).fillna(0)\n        \n        # Calculate user similarity (cosine)\n        print(\"Calculating user similarity matrix...\")\n        user_matrix = self.user_item_matrix.values\n        self.user_similarity = cosine_similarity(user_matrix)\n        \n        self.movies_df = movies_df\n        print(f\"âœ… Trained on {len(self.user_item_matrix)} users\")\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N recommendations for user\"\"\"\n        if user_id not in self.user_item_matrix.index:\n            return []\n        \n        user_idx = self.user_item_matrix.index.get_loc(user_id)\n        \n        # Get similar users\n        user_similarities = self.user_similarity[user_idx]\n        similar_user_indices = user_similarities.argsort()[::-1][1:self.k+1]\n        \n        # Get movies rated by similar users\n        user_ratings = self.user_item_matrix.iloc[user_idx]\n        unrated_movies = user_ratings[user_ratings == 0].index\n        \n        # Predict ratings for unrated movies\n        predictions = {}\n        for movie_id in unrated_movies:\n            if movie_id in self.user_item_matrix.columns:\n                movie_ratings = self.user_item_matrix[movie_id].iloc[similar_user_indices]\n                similarities = user_similarities[similar_user_indices]\n                \n                # Weighted average\n                rated_mask = movie_ratings > 0\n                if rated_mask.sum() > 0:\n                    pred = np.sum(movie_ratings[rated_mask] * similarities[rated_mask]) / np.sum(similarities[rated_mask])\n                    predictions[movie_id] = pred\n        \n        # Get top N\n        top_movies = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:n]\n        \n        # Format recommendations\n        recommendations = []\n        for movie_id, score in top_movies:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'predicted_rating': score\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation\"\"\"\n        return f\"Recommended because users similar to you enjoyed this movie\"\n\n# Train and save\nuser_cf_model = UserUserCF(k=20)\nuser_cf_model.fit(train_ratings, movies_df)\n\nwith open('models/user_cf_model.pkl', 'wb') as f:\n    pickle.dump(user_cf_model, f)\n\nprint(\"âœ… User-User CF model saved!\")\n\n# ============================================================================\n# PART 8: MODEL 3 - ITEM-ITEM COLLABORATIVE FILTERING\n# ============================================================================\n\nprint(\"\\nðŸŽ¬ BUILDING ITEM-ITEM COLLABORATIVE FILTERING...\")\n\nclass ItemItemCF:\n    def __init__(self, k=20):\n        self.k = k\n        self.item_similarity = None\n        self.user_ratings = None\n        self.movies_df = None\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train item-item CF model\"\"\"\n        # Create item-user matrix\n        item_user_matrix = ratings_df.pivot(\n            index='movieId',\n            columns='userId',\n            values='rating'\n        ).fillna(0)\n        \n        # Calculate item similarity\n        print(\"Calculating item similarity matrix...\")\n        self.item_similarity = cosine_similarity(item_user_matrix.values)\n        self.item_similarity = pd.DataFrame(\n            self.item_similarity,\n            index=item_user_matrix.index,\n            columns=item_user_matrix.index\n        )\n        \n        # Store user ratings for predictions\n        self.user_ratings = ratings_df.groupby('userId').apply(\n            lambda x: dict(zip(x['movieId'], x['rating']))\n        ).to_dict()\n        \n        self.movies_df = movies_df\n        print(f\"âœ… Trained on {len(item_user_matrix)} items\")\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N recommendations\"\"\"\n        if user_id not in self.user_ratings:\n            return []\n        \n        user_rated_movies = self.user_ratings[user_id]\n        \n        # Find similar items to user's rated items\n        predictions = {}\n        for rated_movie, rating in user_rated_movies.items():\n            if rated_movie in self.item_similarity.index:\n                similar_movies = self.item_similarity[rated_movie]\n                \n                for movie_id, similarity in similar_movies.items():\n                    if movie_id not in user_rated_movies:\n                        if movie_id not in predictions:\n                            predictions[movie_id] = 0\n                        predictions[movie_id] += similarity * rating\n        \n        # Normalize and get top N\n        top_movies = sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:n]\n        \n        recommendations = []\n        for movie_id, score in top_movies:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'predicted_rating': score\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation\"\"\"\n        if user_id in self.user_ratings:\n            user_movies = list(self.user_ratings[user_id].keys())\n            if movie_id in self.item_similarity.index:\n                similar = self.item_similarity[movie_id][user_movies].nlargest(3)\n                similar_titles = self.movies_df[self.movies_df['movieId'].isin(similar.index)]['title'].tolist()\n                if similar_titles:\n                    return f\"Similar to movies you liked: {', '.join(similar_titles[:2])}\"\n        return \"Based on your viewing history\"\n\n# Train and save\nitem_cf_model = ItemItemCF(k=20)\nitem_cf_model.fit(train_ratings, movies_df)\n\nwith open('models/item_cf_model.pkl', 'wb') as f:\n    pickle.dump(item_cf_model, f)\n\nprint(\"âœ… Item-Item CF model saved!\")\n\n# ============================================================================\n# PART 9: MODEL 4 - MATRIX FACTORIZATION (SVD)\n# ============================================================================\n\nif SURPRISE_AVAILABLE:\n    print(\"\\nðŸ§® BUILDING MATRIX FACTORIZATION MODEL (SVD)...\")\n\n    # Prepare data for Surprise library\n    reader = Reader(rating_scale=(0.5, 5.0))\n    data = Dataset.load_from_df(train_ratings[['userId', 'movieId', 'rating']], reader)\n\n    # Train SVD\n    print(\"Training SVD model...\")\n    svd_model = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02, verbose=True)\n    trainset = data.build_full_trainset()\n    svd_model.fit(trainset)\n\n    # Save model\n    with open('models/svd_model.pkl', 'wb') as f:\n        pickle.dump(svd_model, f)\n\n    print(\"âœ… SVD model saved!\")\n\n    # Evaluation on train set\n    from surprise import accuracy\n\n    predictions = svd_model.test(trainset.build_testset())\n    print(f\"\\nTrain RMSE: {accuracy.rmse(predictions):.4f}\")\n    print(f\"Train MAE: {accuracy.mae(predictions):.4f}\")\nelse:\n    print(\"\\nâš ï¸ SKIPPING SVD MODEL (Surprise library not available)\")\n    svd_model = None\n\n# ============================================================================\n# PART 10: MODEL 5 - CONTENT-BASED FILTERING\n# ============================================================================\n\nprint(\"\\nðŸ“ BUILDING CONTENT-BASED MODEL...\")\n\nclass ContentBasedRecommender:\n    def __init__(self):\n        self.tfidf_matrix = None\n        self.movie_indices = None\n        self.movies_df = None\n        self.user_profiles = {}\n    \n    def fit(self, ratings_df, movies_df):\n        \"\"\"Train content-based model\"\"\"\n        # Create TF-IDF matrix\n        tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\n        self.tfidf_matrix = tfidf.fit_transform(movies_df['content_features'])\n        self.movie_indices = movies_df['movieId'].values\n        self.movies_df = movies_df\n        \n        # Build user profiles based on rated movies\n        print(\"Building user profiles...\")\n        for user_id in ratings_df['userId'].unique():\n            user_ratings = ratings_df[ratings_df['userId'] == user_id]\n            \n            # Get movies rated highly (>= 4.0)\n            liked_movies = user_ratings[user_ratings['rating'] >= 4.0]['movieId'].values\n            \n            # Build profile as average of liked movie features\n            if len(liked_movies) > 0:\n                liked_indices = [np.where(self.movie_indices == mid)[0][0] \n                               for mid in liked_movies if mid in self.movie_indices]\n                \n                if liked_indices:\n                    user_profile = np.array(self.tfidf_matrix[liked_indices].mean(axis=0)).flatten()\n                    self.user_profiles[user_id] = user_profile\n        \n        print(f\"âœ… Created profiles for {len(self.user_profiles)} users\")\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N content-based recommendations\"\"\"\n        if user_id not in self.user_profiles:\n            return []\n        \n        user_profile = self.user_profiles[user_id]\n        \n        # Calculate similarity to all movies\n        similarities = cosine_similarity([user_profile], self.tfidf_matrix)[0]\n        \n        # Get top similar movies\n        top_indices = similarities.argsort()[::-1][:n*2]  # Get more for filtering\n        \n        recommendations = []\n        for idx in top_indices:\n            if len(recommendations) >= n:\n                break\n            \n            movie_id = self.movie_indices[idx]\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id].iloc[0]\n            \n            recommendations.append({\n                'movieId': movie_id,\n                'title': movie_info['title'],\n                'genres': movie_info['genres'],\n                'similarity_score': similarities[idx]\n            })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation\"\"\"\n        movie = self.movies_df[self.movies_df['movieId'] == movie_id]\n        if len(movie) > 0:\n            genres = movie.iloc[0]['genres']\n            return f\"Matches your interest in genres: {genres}\"\n        return \"Based on content similarity to your preferences\"\n\n# Train and save\ncontent_model = ContentBasedRecommender()\ncontent_model.fit(train_ratings, movies_df)\n\nwith open('models/content_model.pkl', 'wb') as f:\n    pickle.dump(content_model, f)\n\nprint(\"âœ… Content-based model saved!\")\n\n# ============================================================================\n# PART 11: HYBRID RECOMMENDER\n# ============================================================================\n\nprint(\"\\nðŸŽ¯ BUILDING HYBRID RECOMMENDER...\")\n\nclass HybridRecommender:\n    def __init__(self, models, weights=None):\n        \"\"\"\n        models: dict of {name: model}\n        weights: dict of {name: weight} (defaults to equal weights)\n        \"\"\"\n        self.models = models\n        self.weights = weights or {name: 1.0/len(models) for name in models}\n        self.movies_df = None\n    \n    def fit(self, movies_df):\n        self.movies_df = movies_df\n        return self\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get hybrid recommendations\"\"\"\n        all_recommendations = {}\n        \n        # Get recommendations from each model\n        for name, model in self.models.items():\n            try:\n                recs = model.recommend(user_id, n=n*2)\n                weight = self.weights[name]\n                \n                for rec in recs:\n                    movie_id = rec['movieId']\n                    score = rec.get('predicted_rating', rec.get('popularity_score', rec.get('similarity_score', 1.0)))\n                    \n                    if movie_id not in all_recommendations:\n                        all_recommendations[movie_id] = {\n                            'score': 0,\n                            'count': 0,\n                            'sources': []\n                        }\n                    \n                    all_recommendations[movie_id]['score'] += score * weight\n                    all_recommendations[movie_id]['count'] += 1\n                    all_recommendations[movie_id]['sources'].append(name)\n            except Exception as e:\n                print(f\"Warning: {name} model failed: {e}\")\n                continue\n        \n        # Normalize scores and get top N\n        for movie_id in all_recommendations:\n            all_recommendations[movie_id]['final_score'] = (\n                all_recommendations[movie_id]['score'] / all_recommendations[movie_id]['count']\n            )\n        \n        top_movies = sorted(\n            all_recommendations.items(),\n            key=lambda x: x[1]['final_score'],\n            reverse=True\n        )[:n]\n        \n        # Format recommendations\n        recommendations = []\n        for movie_id, data in top_movies:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'hybrid_score': data['final_score'],\n                    'sources': data['sources']\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id, sources):\n        \"\"\"Generate hybrid explanation\"\"\"\n        explanations = []\n        for source in sources:\n            if source in self.models:\n                exp = self.models[source].explain(user_id, movie_id)\n                explanations.append(f\"{source}: {exp}\")\n        \n        return \" | \".join(explanations)\n\n# Create hybrid model\nhybrid_models = {\n    'user_cf': user_cf_model,\n    'item_cf': item_cf_model,\n    'content': content_model,\n}\n\nhybrid_weights = {\n    'user_cf': 0.4,\n    'item_cf': 0.4,\n    'content': 0.2\n}\n\nhybrid_model = HybridRecommender(hybrid_models, hybrid_weights)\nhybrid_model.fit(movies_df)\n\nwith open('models/hybrid_model.pkl', 'wb') as f:\n    pickle.dump(hybrid_model, f)\n\nprint(\"âœ… Hybrid model saved!\")\n\n# ============================================================================\n# PART 12: EVALUATION METRICS\n# ============================================================================\n\nprint(\"\\nðŸ“ˆ CALCULATING EVALUATION METRICS...\")\n\ndef precision_at_k(recommended, relevant, k=10):\n    \"\"\"Precision@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = len([r for r in recommended_k if r in relevant_set])\n    return hits / k if k > 0 else 0\n\ndef recall_at_k(recommended, relevant, k=10):\n    \"\"\"Recall@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = len([r for r in recommended_k if r in relevant_set])\n    return hits / len(relevant_set) if len(relevant_set) > 0 else 0\n\ndef ndcg_at_k(recommended, relevant, k=10):\n    \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    \n    dcg = sum([1.0 / np.log2(idx + 2) if rec in relevant_set else 0 \n               for idx, rec in enumerate(recommended_k)])\n    \n    idcg = sum([1.0 / np.log2(idx + 2) for idx in range(min(len(relevant), k))])\n    \n    return dcg / idcg if idcg > 0 else 0\n\ndef evaluate_model(model, test_ratings, k=10):\n    \"\"\"Evaluate a recommendation model\"\"\"\n    precisions = []\n    recalls = []\n    ndcgs = []\n    \n    # Sample users for faster evaluation\n    test_users = test_ratings['userId'].unique()[:100]  # Sample 100 users\n    \n    for user_id in test_users:\n        # Get user's test items (relevant items)\n        relevant_items = test_ratings[test_ratings['userId'] == user_id]['movieId'].tolist()\n        \n        if len(relevant_items) == 0:\n            continue\n        \n        try:\n            # Get recommendations\n            recs = model.recommend(user_id, n=k)\n            recommended_items = [r['movieId'] for r in recs]\n            \n            if len(recommended_items) > 0:\n                precisions.append(precision_at_k(recommended_items, relevant_items, k))\n                recalls.append(recall_at_k(recommended_items, relevant_items, k))\n                ndcgs.append(ndcg_at_k(recommended_items, relevant_items, k))\n        except:\n            continue\n    \n    return {\n        'Precision@K': np.mean(precisions) if precisions else 0,\n        'Recall@K': np.mean(recalls) if recalls else 0,\n        'NDCG@K': np.mean(ndcgs) if ndcgs else 0\n    }\n\n# Evaluate models\nprint(\"\\nðŸŽ¯ Evaluating models on test set...\")\n\nresults = {}\n\nprint(\"\\n1. Evaluating Hybrid Model...\")\nresults['Hybrid'] = evaluate_model(hybrid_model, test_ratings, k=10)\n\nprint(\"\\n2. Evaluating User-User CF...\")\nresults['User-User CF'] = evaluate_model(user_cf_model, test_ratings, k=10)\n\nprint(\"\\n3. Evaluating Item-Item CF...\")\nresults['Item-Item CF'] = evaluate_model(item_cf_model, test_ratings, k=10)\n\nprint(\"\\n4. Evaluating Content-Based...\")\nresults['Content-Based'] = evaluate_model(content_model, test_ratings, k=10)\n\n# Display results\nprint(\"\\n\" + \"=\"*70)\nprint(\"EVALUATION RESULTS (K=10)\")\nprint(\"=\"*70)\n\nresults_df = pd.DataFrame(results).T\nprint(results_df.to_string())\n\n# Save results\nresults_df.to_csv('evaluation_results.csv')\nprint(\"\\nâœ… Results saved to 'evaluation_results.csv'\")\n\n# ============================================================================\n# PART 13: DIVERSITY & NOVELTY METRICS\n# ============================================================================\n\nprint(\"\\nðŸŒˆ CALCULATING DIVERSITY METRICS...\")\n\ndef calculate_diversity_metrics(model, test_users, movies_df, k=10):\n    \"\"\"Calculate diversity and novelty metrics\"\"\"\n    \n    # Catalog coverage\n    all_recommended = set()\n    total_catalog = len(movies_df)\n    \n    # Intra-list diversity\n    intra_diversities = []\n    \n    for user_id in test_users[:50]:  # Sample for speed\n        try:\n            recs = model.recommend(user_id, n=k)\n            rec_ids = [r['movieId'] for r in recs]\n            \n            all_recommended.update(rec_ids)\n            \n            # Calculate genre diversity\n            rec_genres = []\n            for movie_id in rec_ids:\n                movie = movies_df[movies_df['movieId'] == movie_id]\n                if len(movie) > 0:\n                    genres = movie.iloc[0]['genres'].split('|')\n                    rec_genres.extend(genres)\n            \n            # Unique genres / total items\n            if len(rec_ids) > 0:\n                diversity = len(set(rec_genres)) / len(rec_ids)\n                intra_diversities.append(diversity)\n        except:\n            continue\n    \n    catalog_coverage = len(all_recommended) / total_catalog\n    avg_intra_diversity = np.mean(intra_diversities) if intra_diversities else 0\n    \n    return {\n        'Catalog Coverage': catalog_coverage,\n        'Intra-List Diversity': avg_intra_diversity\n    }\n\n# Calculate for hybrid model\ntest_users = test_ratings['userId'].unique()\ndiversity_metrics = calculate_diversity_metrics(hybrid_model, test_users, movies_df, k=10)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"DIVERSITY METRICS (Hybrid Model)\")\nprint(\"=\"*70)\nfor metric, value in diversity_metrics.items():\n    print(f\"{metric}: {value:.4f}\")\n\n# Save diversity metrics\nwith open('diversity_metrics.json', 'w') as f:\n    json.dump(diversity_metrics, f, indent=2)\n\nprint(\"\\nâœ… Diversity metrics saved to 'diversity_metrics.json'\")\n\n# ============================================================================\n# PART 14: EXPLAINABILITY EXAMPLES\n# ============================================================================\n\nprint(\"\\nðŸ’¡ GENERATING EXPLANATION EXAMPLES...\")\n\n# Get a sample user\nsample_user = train_ratings['userId'].iloc[0]\n\nprint(f\"\\nðŸ“Œ Sample Recommendations for User {sample_user}:\")\nprint(\"=\"*70)\n\n# Get hybrid recommendations\nhybrid_recs = hybrid_model.recommend(sample_user, n=5)\n\nfor i, rec in enumerate(hybrid_recs, 1):\n    print(f\"\\n{i}. {rec['title']}\")\n    print(f\"   Genres: {rec['genres']}\")\n    print(f\"   Score: {rec['hybrid_score']:.4f}\")\n    print(f\"   Sources: {', '.join(rec['sources'])}\")\n    \n    # Generate explanation\n    explanation = hybrid_model.explain(sample_user, rec['movieId'], rec['sources'])\n    print(f\"   ðŸ“– Why: {explanation}\")\n\n# ============================================================================\n# PART 15: SAVE MODEL METADATA\n# ============================================================================\n\nprint(\"\\nðŸ’¾ SAVING MODEL METADATA...\")\n\nmetadata = {\n    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n    'dataset_stats': {\n        'total_ratings': len(ratings_df),\n        'total_users': ratings_df['userId'].nunique(),\n        'total_movies': ratings_df['movieId'].nunique(),\n        'train_size': len(train_ratings),\n        'test_size': len(test_ratings)\n    },\n    'models': {\n        'popularity': 'PopularityRecommender',\n        'user_cf': 'UserUserCF(k=20)',\n        'item_cf': 'ItemItemCF(k=20)',\n        'svd': 'SVD(factors=100)' if SURPRISE_AVAILABLE else 'Not trained',\n        'content': 'ContentBasedRecommender',\n        'hybrid': 'HybridRecommender(weights=user_cf:0.4, item_cf:0.4, content:0.2)'\n    },\n    'evaluation': results,\n    'diversity': diversity_metrics,\n    'surprise_library_available': SURPRISE_AVAILABLE\n}\n\nwith open('models/model_metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\n\nprint(\"âœ… Metadata saved to 'models/model_metadata.json'\")\n\n# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŽ‰ REELSENSE TRAINING PIPELINE COMPLETE!\")\nprint(\"=\"*70)\nprint(\"\\nðŸ“ Files Generated:\")\nprint(\"  âœ“ models/popularity_model.pkl\")\nprint(\"  âœ“ models/user_cf_model.pkl\")\nprint(\"  âœ“ models/item_cf_model.pkl\")\nif SURPRISE_AVAILABLE:\n    print(\"  âœ“ models/svd_model.pkl\")\nelse:\n    print(\"  âš  models/svd_model.pkl (SKIPPED - Surprise library issue)\")\nprint(\"  âœ“ models/content_model.pkl\")\nprint(\"  âœ“ models/hybrid_model.pkl\")\nprint(\"  âœ“ models/model_metadata.json\")\nprint(\"  âœ“ evaluation_results.csv\")\nprint(\"  âœ“ diversity_metrics.json\")\nprint(\"  âœ“ eda_plots/ (6 visualization files)\")\n\nprint(\"\\nðŸ“Š Best Model Performance:\")\nbest_model = max(results.items(), key=lambda x: x[1]['NDCG@K'])\nprint(f\"  {best_model[0]}: NDCG@10 = {best_model[1]['NDCG@K']:.4f}\")\n\nif not SURPRISE_AVAILABLE:\n    print(\"\\nâš ï¸ NOTE: SVD model was skipped due to NumPy compatibility.\")\n    print(\"   To enable SVD, please restart your kernel after running this script once.\")\n\nprint(\"\\nâœ¨ Next Step: Deploy with Streamlit UI!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-06T15:48:38.752079Z","iopub.execute_input":"2026-02-06T15:48:38.752791Z","iopub.status.idle":"2026-02-06T15:56:00.692282Z","shell.execute_reply.started":"2026-02-06T15:48:38.752761Z","shell.execute_reply":"2026-02-06T15:56:00.691634Z"}},"outputs":[{"name":"stderr","text":"\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_55/2301192875.py\", line 48, in <cell line: 0>\n    from surprise import SVD, Dataset, Reader\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\", line 6, in <module>\n    from .prediction_algorithms import (\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n    from .algo_base import AlgoBase\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n    from .. import similarities as sims\n","output_type":"stream"},{"name":"stdout","text":"âš ï¸ Warning: Surprise library not available: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).\n   SVD model will be skipped. Other models will work fine.\nâœ… All libraries imported successfully!\nðŸ“Š Dataset Statistics:\nRatings: 100,836 rows\nMovies: 9,742 rows\nTags: 3,683 rows\nLinks: 9,742 rows\n\nUsers: 610\nMovies rated: 9,724\n\nðŸ“‹ Sample Data:\n\nRatings:\n   userId  movieId  rating  timestamp\n0       1        1     4.0  964982703\n1       1        3     4.0  964981247\n2       1        6     4.0  964982224\n3       1       47     5.0  964983815\n4       1       50     5.0  964982931\n\nMovies:\n   movieId                               title  \\\n0        1                    Toy Story (1995)   \n1        2                      Jumanji (1995)   \n2        3             Grumpier Old Men (1995)   \n3        4            Waiting to Exhale (1995)   \n4        5  Father of the Bride Part II (1995)   \n\n                                        genres  \n0  Adventure|Animation|Children|Comedy|Fantasy  \n1                   Adventure|Children|Fantasy  \n2                               Comedy|Romance  \n3                         Comedy|Drama|Romance  \n4                                       Comedy  \n\nTags:\n   userId  movieId              tag   timestamp\n0       2    60756            funny  1445714994\n1       2    60756  Highly quotable  1445714996\n2       2    60756     will ferrell  1445714992\n3       2    89774     Boxing story  1445715207\n4       2    89774              MMA  1445715200\n\nðŸ§¹ PREPROCESSING DATA...\nâœ… Preprocessing complete!\n\nðŸ“Š PERFORMING EDA...\nâœ… EDA plots saved to 'eda_plots/' directory\n\nðŸ”€ CREATING TRAIN-TEST SPLIT...\nTrain set: 100,226 ratings\nTest set: 610 ratings\nTrain users: 610\nTest users: 610\n\nðŸŒŸ BUILDING POPULARITY-BASED MODEL...\nâœ… Trained on 445 popular movies\nâœ… Popularity model saved!\n\nðŸ‘¥ BUILDING USER-USER COLLABORATIVE FILTERING...\nCalculating user similarity matrix...\nâœ… Trained on 610 users\nâœ… User-User CF model saved!\n\nðŸŽ¬ BUILDING ITEM-ITEM COLLABORATIVE FILTERING...\nCalculating item similarity matrix...\nâœ… Trained on 9703 items\nâœ… Item-Item CF model saved!\n\nâš ï¸ SKIPPING SVD MODEL (Surprise library not available)\n\nðŸ“ BUILDING CONTENT-BASED MODEL...\nBuilding user profiles...\nâœ… Created profiles for 609 users\nâœ… Content-based model saved!\n\nðŸŽ¯ BUILDING HYBRID RECOMMENDER...\nâœ… Hybrid model saved!\n\nðŸ“ˆ CALCULATING EVALUATION METRICS...\n\nðŸŽ¯ Evaluating models on test set...\n\n1. Evaluating Hybrid Model...\n\n2. Evaluating User-User CF...\n\n3. Evaluating Item-Item CF...\n\n4. Evaluating Content-Based...\n\n======================================================================\nEVALUATION RESULTS (K=10)\n======================================================================\n               Precision@K  Recall@K    NDCG@K\nHybrid               0.014      0.14  0.068866\nUser-User CF         0.005      0.05  0.023957\nItem-Item CF         0.013      0.13  0.065894\nContent-Based        0.001      0.01  0.003010\n\nâœ… Results saved to 'evaluation_results.csv'\n\nðŸŒˆ CALCULATING DIVERSITY METRICS...\n\n======================================================================\nDIVERSITY METRICS (Hybrid Model)\n======================================================================\nCatalog Coverage: 0.0074\nIntra-List Diversity: 1.2140\n\nâœ… Diversity metrics saved to 'diversity_metrics.json'\n\nðŸ’¡ GENERATING EXPLANATION EXAMPLES...\n\nðŸ“Œ Sample Recommendations for User 429:\n======================================================================\n\n1. Fugitive, The (1993)\n   Genres: Thriller\n   Score: 32.4795\n   Sources: item_cf\n   ðŸ“– Why: item_cf: Similar to movies you liked: Clear and Present Danger (1994), True Lies (1994)\n\n2. Batman Forever (1995)\n   Genres: Action|Adventure|Comedy|Crime\n   Score: 32.1209\n   Sources: item_cf\n   ðŸ“– Why: item_cf: Similar to movies you liked: True Lies (1994), Cliffhanger (1993)\n\n3. Firm, The (1993)\n   Genres: Drama|Thriller\n   Score: 31.6352\n   Sources: item_cf\n   ðŸ“– Why: item_cf: Similar to movies you liked: Crimson Tide (1995), Outbreak (1995)\n\n4. Client, The (1994)\n   Genres: Drama|Mystery|Thriller\n   Score: 31.6056\n   Sources: item_cf\n   ðŸ“– Why: item_cf: Similar to movies you liked: Crimson Tide (1995), Disclosure (1994)\n\n5. Speed (1994)\n   Genres: Action|Romance|Thriller\n   Score: 30.3488\n   Sources: item_cf\n   ðŸ“– Why: item_cf: Similar to movies you liked: Die Hard: With a Vengeance (1995), True Lies (1994)\n\nðŸ’¾ SAVING MODEL METADATA...\nâœ… Metadata saved to 'models/model_metadata.json'\n\n======================================================================\nðŸŽ‰ REELSENSE TRAINING PIPELINE COMPLETE!\n======================================================================\n\nðŸ“ Files Generated:\n  âœ“ models/popularity_model.pkl\n  âœ“ models/user_cf_model.pkl\n  âœ“ models/item_cf_model.pkl\n  âš  models/svd_model.pkl (SKIPPED - Surprise library issue)\n  âœ“ models/content_model.pkl\n  âœ“ models/hybrid_model.pkl\n  âœ“ models/model_metadata.json\n  âœ“ evaluation_results.csv\n  âœ“ diversity_metrics.json\n  âœ“ eda_plots/ (6 visualization files)\n\nðŸ“Š Best Model Performance:\n  Hybrid: NDCG@10 = 0.0689\n\nâš ï¸ NOTE: SVD model was skipped due to NumPy compatibility.\n   To enable SVD, please restart your kernel after running this script once.\n\nâœ¨ Next Step: Deploy with Streamlit UI!\n======================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nReelSense: SVD-Only Training Script\nHandles NumPy 2.x compatibility automatically\n\"\"\"\n\nimport subprocess\nimport sys\n\nprint(\"=\"*70)\nprint(\"ðŸŽ¬ REELSENSE - SVD MODEL TRAINING\")\nprint(\"=\"*70)\n\n# ============================================================================\n# STEP 1: FIX NUMPY COMPATIBILITY\n# ============================================================================\n\nprint(\"\\nðŸ”§ Checking NumPy compatibility...\")\n\n# try:\n#     import numpy as np\n#     numpy_version = np.__version__\n#     major_version = int(numpy_version.split('.')[0])\n    \n#     print(f\"Current NumPy version: {numpy_version}\")\n    \n#     if major_version >= 2:\n#         print(\"âš ï¸  NumPy 2.x detected - Surprise library requires NumPy 1.x\")\n#         print(\"ðŸ“¥ Downgrading to NumPy 1.x...\")\n        \n#         subprocess.check_call([\n#             sys.executable, \"-m\", \"pip\", \"install\", \n#             \"numpy<2\", \"--quiet\"\n#         ])\n        \n#         print(\"âœ… NumPy downgraded successfully!\")\n#         print(\"ðŸ”„ Please RESTART your kernel/runtime and run this script again.\")\n#         print(\"\\nIn Google Colab: Runtime -> Restart runtime\")\n#         print(\"In Kaggle: Session -> Restart session\")\n#         print(\"In Jupyter: Kernel -> Restart\")\n#         sys.exit(0)\n#     else:\n#         print(f\"âœ… NumPy {numpy_version} is compatible!\")\n        \n# except Exception as e:\n#     print(f\"Error checking NumPy: {e}\")\n#     print(\"Continuing anyway...\")\n\n# ============================================================================\n# STEP 2: IMPORTS\n# ============================================================================\n\nprint(\"\\nðŸ“¦ Importing libraries...\")\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nimport pickle\nimport json\nimport os\n\n# Try importing Surprise\ntry:\n    from surprise import SVD, Dataset, Reader\n    from surprise.model_selection import cross_validate\n    from surprise import accuracy\n    SURPRISE_AVAILABLE = True\n    print(\"âœ… Surprise library loaded successfully!\")\nexcept ImportError as e:\n    print(f\"âŒ ERROR: Surprise library could not be loaded\")\n    print(f\"   Error details: {e}\")\n    print(\"\\nðŸ’¡ Solution:\")\n    print(\"   1. Restart your kernel/runtime\")\n    print(\"   2. Run this script again\")\n    sys.exit(1)\n\n# ============================================================================\n# STEP 3: LOAD DATA\n# ============================================================================\n\nprint(\"\\nðŸ“‚ Loading datasets...\")\n\n# Update these paths based on your environment\n# For Kaggle:\nratings_df = pd.read_csv('/kaggle/input/movie-recommendation/ratings.csv')\nmovies_df = pd.read_csv('/kaggle/input/movie-recommendation/movies.csv')\n\n# For local files, use:\n# ratings_df = pd.read_csv('ratings.csv')\n# movies_df = pd.read_csv('movies.csv')\n\nprint(f\"âœ… Loaded {len(ratings_df):,} ratings\")\nprint(f\"âœ… Loaded {len(movies_df):,} movies\")\n\n# ============================================================================\n# STEP 4: TRAIN-TEST SPLIT\n# ============================================================================\n\nprint(\"\\nðŸ”€ Creating train-test split...\")\n\n# Sort by timestamp\nratings_df['timestamp'] = pd.to_datetime(ratings_df['timestamp'], unit='s')\nratings_df = ratings_df.sort_values('timestamp')\n\ndef leave_last_n_split(df, n=1):\n    \"\"\"Leave-last-N split per user\"\"\"\n    train_list = []\n    test_list = []\n    \n    for user_id in df['userId'].unique():\n        user_data = df[df['userId'] == user_id].sort_values('timestamp')\n        \n        if len(user_data) > n:\n            train_list.append(user_data.iloc[:-n])\n            test_list.append(user_data.iloc[-n:])\n        else:\n            train_list.append(user_data)\n    \n    train_df = pd.concat(train_list, ignore_index=True)\n    test_df = pd.concat(test_list, ignore_index=True)\n    \n    return train_df, test_df\n\ntrain_ratings, test_ratings = leave_last_n_split(ratings_df, n=1)\n\nprint(f\"âœ… Train set: {len(train_ratings):,} ratings\")\nprint(f\"âœ… Test set: {len(test_ratings):,} ratings\")\n\n# ============================================================================\n# STEP 5: PREPARE DATA FOR SURPRISE\n# ============================================================================\n\nprint(\"\\nðŸŽ¯ Preparing data for SVD...\")\n\n# Define rating scale\nreader = Reader(rating_scale=(0.5, 5.0))\n\n# Create Surprise dataset from train data\ntrain_data = Dataset.load_from_df(\n    train_ratings[['userId', 'movieId', 'rating']], \n    reader\n)\n\n# Build full trainset\ntrainset = train_data.build_full_trainset()\n\nprint(f\"âœ… Trainset ready: {trainset.n_users} users, {trainset.n_items} items\")\n\n# ============================================================================\n# STEP 6: TRAIN SVD MODEL\n# ============================================================================\n\nprint(\"\\nðŸ§® Training SVD Model...\")\nprint(\"   Hyperparameters:\")\nprint(\"   - Factors: 100\")\nprint(\"   - Epochs: 20\")\nprint(\"   - Learning Rate: 0.005\")\nprint(\"   - Regularization: 0.02\")\nprint()\n\n# Initialize SVD\nsvd_model = SVD(\n    n_factors=100,\n    n_epochs=20,\n    lr_all=0.005,\n    reg_all=0.02,\n    verbose=True\n)\n\n# Train the model\nsvd_model.fit(trainset)\n\nprint(\"\\nâœ… SVD model training complete!\")\n\n# ============================================================================\n# STEP 7: EVALUATION ON TRAIN SET\n# ============================================================================\n\nprint(\"\\nðŸ“Š Evaluating on train set...\")\n\ntrain_predictions = svd_model.test(trainset.build_testset())\ntrain_rmse = accuracy.rmse(train_predictions, verbose=False)\ntrain_mae = accuracy.mae(train_predictions, verbose=False)\n\nprint(f\"   Train RMSE: {train_rmse:.4f}\")\nprint(f\"   Train MAE: {train_mae:.4f}\")\n\n# ============================================================================\n# STEP 8: EVALUATION ON TEST SET\n# ============================================================================\n\nprint(\"\\nðŸ“Š Evaluating on test set...\")\n\n# Convert test data to Surprise format\ntest_data = Dataset.load_from_df(\n    test_ratings[['userId', 'movieId', 'rating']], \n    reader\n)\n\n# Build testset\ntestset = test_data.build_full_trainset().build_testset()\n\n# Make predictions\ntest_predictions = svd_model.test(testset)\n\n# Calculate metrics\ntest_rmse = accuracy.rmse(test_predictions, verbose=False)\ntest_mae = accuracy.mae(test_predictions, verbose=False)\n\nprint(f\"   Test RMSE: {test_rmse:.4f}\")\nprint(f\"   Test MAE: {test_mae:.4f}\")\n\n# ============================================================================\n# STEP 9: RANKING METRICS (Precision, Recall, NDCG)\n# ============================================================================\n\nprint(\"\\nðŸ“ˆ Calculating ranking metrics...\")\n\ndef precision_at_k(recommended, relevant, k=10):\n    \"\"\"Precision@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = len([r for r in recommended_k if r in relevant_set])\n    return hits / k if k > 0 else 0\n\ndef recall_at_k(recommended, relevant, k=10):\n    \"\"\"Recall@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    hits = len([r for r in recommended_k if r in relevant_set])\n    return hits / len(relevant_set) if len(relevant_set) > 0 else 0\n\ndef ndcg_at_k(recommended, relevant, k=10):\n    \"\"\"Normalized Discounted Cumulative Gain@K\"\"\"\n    recommended_k = recommended[:k]\n    relevant_set = set(relevant)\n    \n    dcg = sum([1.0 / np.log2(idx + 2) if rec in relevant_set else 0 \n               for idx, rec in enumerate(recommended_k)])\n    \n    idcg = sum([1.0 / np.log2(idx + 2) for idx in range(min(len(relevant), k))])\n    \n    return dcg / idcg if idcg > 0 else 0\n\n# Evaluate on sample of test users\ntest_users = test_ratings['userId'].unique()[:100]  # Sample 100 users\n\nprecisions = []\nrecalls = []\nndcgs = []\n\nprint(\"   Evaluating 100 sample users...\")\n\nfor user_id in test_users:\n    # Get relevant items (items user rated in test set)\n    relevant_items = test_ratings[test_ratings['userId'] == user_id]['movieId'].tolist()\n    \n    if len(relevant_items) == 0:\n        continue\n    \n    # Get all items user hasn't rated in train set\n    user_train_items = set(train_ratings[train_ratings['userId'] == user_id]['movieId'])\n    all_items = set(train_ratings['movieId'].unique())\n    unrated_items = list(all_items - user_train_items)\n    \n    if len(unrated_items) == 0:\n        continue\n    \n    # Predict ratings for unrated items\n    predictions_list = []\n    for item_id in unrated_items:\n        pred = svd_model.predict(user_id, item_id)\n        predictions_list.append((item_id, pred.est))\n    \n    # Sort by predicted rating\n    predictions_list.sort(key=lambda x: x[1], reverse=True)\n    recommended_items = [item_id for item_id, _ in predictions_list[:10]]\n    \n    # Calculate metrics\n    precisions.append(precision_at_k(recommended_items, relevant_items, k=10))\n    recalls.append(recall_at_k(recommended_items, relevant_items, k=10))\n    ndcgs.append(ndcg_at_k(recommended_items, relevant_items, k=10))\n\n# Calculate average metrics\navg_precision = np.mean(precisions) if precisions else 0\navg_recall = np.mean(recalls) if recalls else 0\navg_ndcg = np.mean(ndcgs) if ndcgs else 0\n\nprint(f\"\\n   Precision@10: {avg_precision:.4f}\")\nprint(f\"   Recall@10: {avg_recall:.4f}\")\nprint(f\"   NDCG@10: {avg_ndcg:.4f}\")\n\n# ============================================================================\n# STEP 10: CREATE WRAPPER CLASS FOR RECOMMENDATIONS\n# ============================================================================\n\nprint(\"\\nðŸŽ Creating recommendation wrapper...\")\n\nclass SVDRecommender:\n    \"\"\"Wrapper class for SVD model with recommendation interface\"\"\"\n    \n    def __init__(self, svd_model, trainset, movies_df):\n        self.svd_model = svd_model\n        self.trainset = trainset\n        self.movies_df = movies_df\n        \n        # Create mapping of internal to external IDs\n        self.user_map = {iid: uid for uid, iid in trainset._raw2inner_id_users.items()}\n        self.item_map = {iid: uid for uid, iid in trainset._raw2inner_id_items.items()}\n    \n    def recommend(self, user_id, n=10):\n        \"\"\"Get top N recommendations for user\"\"\"\n        # Get items user has already rated\n        try:\n            user_inner_id = self.trainset.to_inner_uid(user_id)\n            user_items = set([iid for (iid, _) in self.trainset.ur[user_inner_id]])\n            user_rated_movies = [self.trainset.to_raw_iid(iid) for iid in user_items]\n        except:\n            user_rated_movies = []\n        \n        # Get all items\n        all_items = self.movies_df['movieId'].unique()\n        unrated_items = [item for item in all_items if item not in user_rated_movies]\n        \n        # Predict ratings for unrated items\n        predictions = []\n        for item_id in unrated_items:\n            pred = self.svd_model.predict(user_id, item_id)\n            predictions.append((item_id, pred.est))\n        \n        # Sort and get top N\n        predictions.sort(key=lambda x: x[1], reverse=True)\n        top_items = predictions[:n]\n        \n        # Format recommendations\n        recommendations = []\n        for movie_id, pred_rating in top_items:\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\n            if len(movie_info) > 0:\n                movie_info = movie_info.iloc[0]\n                recommendations.append({\n                    'movieId': movie_id,\n                    'title': movie_info['title'],\n                    'genres': movie_info['genres'],\n                    'predicted_rating': pred_rating\n                })\n        \n        return recommendations\n    \n    def explain(self, user_id, movie_id):\n        \"\"\"Generate explanation for recommendation\"\"\"\n        return \"Recommended based on latent factor analysis of your rating patterns\"\n\n# Create recommender\nsvd_recommender = SVDRecommender(svd_model, trainset, movies_df)\n\nprint(\"âœ… Recommender wrapper created!\")\n\n# ============================================================================\n# STEP 11: TEST RECOMMENDATIONS\n# ============================================================================\n\nprint(\"\\nðŸŽ¬ Testing recommendations...\")\n\nsample_user = train_ratings['userId'].iloc[0]\nrecommendations = svd_recommender.recommend(sample_user, n=5)\n\nprint(f\"\\nTop 5 Recommendations for User {sample_user}:\")\nprint(\"=\"*70)\nfor i, rec in enumerate(recommendations, 1):\n    print(f\"{i}. {rec['title']}\")\n    print(f\"   Genres: {rec['genres']}\")\n    print(f\"   Predicted Rating: {rec['predicted_rating']:.2f}/5.0\")\n    print()\n\n# ============================================================================\n# STEP 12: SAVE MODEL AND RESULTS\n# ============================================================================\n\nprint(\"ðŸ’¾ Saving model and results...\")\n\n# Create output directory\nos.makedirs('models', exist_ok=True)\n\n# Save SVD model\nwith open('models/svd_model.pkl', 'wb') as f:\n    pickle.dump(svd_model, f)\nprint(\"âœ… Saved: models/svd_model.pkl\")\n\n# Save recommender wrapper\nwith open('models/svd_recommender.pkl', 'wb') as f:\n    pickle.dump(svd_recommender, f)\nprint(\"âœ… Saved: models/svd_recommender.pkl\")\n\n# Save evaluation results\nresults = {\n    'model': 'SVD',\n    'hyperparameters': {\n        'n_factors': 100,\n        'n_epochs': 20,\n        'lr_all': 0.005,\n        'reg_all': 0.02\n    },\n    'train_metrics': {\n        'RMSE': float(train_rmse),\n        'MAE': float(train_mae)\n    },\n    'test_metrics': {\n        'RMSE': float(test_rmse),\n        'MAE': float(test_mae),\n        'Precision@10': float(avg_precision),\n        'Recall@10': float(avg_recall),\n        'NDCG@10': float(avg_ndcg)\n    },\n    'dataset_stats': {\n        'total_ratings': len(ratings_df),\n        'train_size': len(train_ratings),\n        'test_size': len(test_ratings),\n        'n_users': trainset.n_users,\n        'n_items': trainset.n_items\n    },\n    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n}\n\nwith open('svd_evaluation_results.json', 'w') as f:\n    json.dump(results, indent=2, fp=f)\nprint(\"âœ… Saved: svd_evaluation_results.json\")\n\n# Save as CSV for easy viewing\nresults_df = pd.DataFrame({\n    'Metric': ['RMSE', 'MAE', 'Precision@10', 'Recall@10', 'NDCG@10'],\n    'Train': [train_rmse, train_mae, '-', '-', '-'],\n    'Test': [test_rmse, test_mae, avg_precision, avg_recall, avg_ndcg]\n})\nresults_df.to_csv('svd_metrics.csv', index=False)\nprint(\"âœ… Saved: svd_metrics.csv\")\n\n# ============================================================================\n# FINAL SUMMARY\n# ============================================================================\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ðŸŽ‰ SVD MODEL TRAINING COMPLETE!\")\nprint(\"=\"*70)\n\nprint(\"\\nðŸ“Š Final Metrics:\")\nprint(f\"   Test RMSE: {test_rmse:.4f}\")\nprint(f\"   Test MAE: {test_mae:.4f}\")\nprint(f\"   Precision@10: {avg_precision:.4f}\")\nprint(f\"   Recall@10: {avg_recall:.4f}\")\nprint(f\"   NDCG@10: {avg_ndcg:.4f}\")\n\nprint(\"\\nðŸ“ Generated Files:\")\nprint(\"   âœ… models/svd_model.pkl\")\nprint(\"   âœ… models/svd_recommender.pkl\")\nprint(\"   âœ… svd_evaluation_results.json\")\nprint(\"   âœ… svd_metrics.csv\")\n\nprint(\"\\nðŸ’¡ Usage Example:\")\nprint(\"\"\"\n# Load the model\nimport pickle\nwith open('models/svd_recommender.pkl', 'rb') as f:\n    svd_rec = pickle.load(f)\n\n# Get recommendations\nrecommendations = svd_rec.recommend(user_id=1, n=10)\n\"\"\")\n\nprint(\"\\nâœ¨ All done!\")\nprint(\"=\"*70)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:46:01.648628Z","iopub.execute_input":"2026-02-07T06:46:01.648957Z","iopub.status.idle":"2026-02-07T06:46:02.718897Z","shell.execute_reply.started":"2026-02-07T06:46:01.648930Z","shell.execute_reply":"2026-02-07T06:46:02.717961Z"}},"outputs":[{"name":"stdout","text":"======================================================================\nðŸŽ¬ REELSENSE - SVD MODEL TRAINING\n======================================================================\n\nðŸ”§ Checking NumPy compatibility...\n\nðŸ“¦ Importing libraries...\n","output_type":"stream"},{"name":"stderr","text":"\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\nTraceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n  File \"<frozen runpy>\", line 88, in _run_code\n  File \"/usr/local/lib/python3.12/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n    ColabKernelApp.launch_instance()\n  File \"/usr/local/lib/python3.12/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n    self.io_loop.start()\n  File \"/usr/local/lib/python3.12/dist-packages/tornado/platform/asyncio.py\", line 211, in start\n    self.asyncio_loop.run_forever()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n    self._run_once()\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n    handle._run()\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n    await self.process_one()\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n    await dispatch(*args)\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n    await result\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n    reply_content = await reply_content\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n    res = shell.run_cell(\n  File \"/usr/local/lib/python3.12/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n    result = self._run_cell(\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n    return runner(coro)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_55/4168225772.py\", line 67, in <cell line: 0>\n    from surprise import SVD, Dataset, Reader\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\", line 6, in <module>\n    from .prediction_algorithms import (\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n    from .algo_base import AlgoBase\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n    from .. import similarities as sims\nERROR:root:Internal Python error in the inspect module.\nBelow is the traceback from this internal error.\n\n","output_type":"stream"},{"name":"stdout","text":"âŒ ERROR: Surprise library could not be loaded\n   Error details: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).\n\nðŸ’¡ Solution:\n   1. Restart your kernel/runtime\n   2. Run this script again\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_55/4168225772.py\", line 67, in <cell line: 0>\n    from surprise import SVD, Dataset, Reader\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\", line 6, in <module>\n    from .prediction_algorithms import (\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\", line 23, in <module>\n    from .algo_base import AlgoBase\n  File \"/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\", line 8, in <module>\n    from .. import similarities as sims\n  File \"surprise/similarities.pyx\", line 1, in init surprise.similarities\nImportError: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_55/4168225772.py\", line 78, in <cell line: 0>\n    sys.exit(1)\nSystemExit: 1\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/inspect.py\", line 1769, in getinnerframes\n    traceback_info = getframeinfo(tb, context)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/inspect.py\", line 1701, in getframeinfo\n    lineno = frame.f_lineno\n             ^^^^^^^^^^^^^^\nAttributeError: 'tuple' object has no attribute 'f_lineno'\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/4168225772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0msurprise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcross_validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m from .prediction_algorithms import (\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mAlgoBase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0malgo_base\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlgoBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbaseline_only\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaselineOnly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/prediction_algorithms/algo_base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimilarities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize_baselines\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbaseline_als\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/surprise/similarities.pyx\u001b[0m in \u001b[0;36minit surprise.similarities\u001b[0;34m()\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/4168225772.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"   2. Run this script again\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mSystemExit\u001b[0m: 1","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[1;32m   2091\u001b[0m                            'the full traceback.\\n']\n\u001b[0;32m-> 2092\u001b[0;31m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[1;32m   2093\u001b[0m                                                                      value))\n\u001b[1;32m   2094\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[0;34m(self, etype, value)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \"\"\"\n\u001b[0;32m--> 754\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             out_list = (\n\u001b[0;32m--> 629\u001b[0;31m                 self.structured_traceback(\n\u001b[0m\u001b[1;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0metb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"],"ename":"TypeError","evalue":"object of type 'NoneType' has no len()","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"!pip uninstall -y numpy\n!pip install \"numpy<2.0\"\n!pip install scikit-surprise\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-07T06:47:00.892984Z","iopub.execute_input":"2026-02-07T06:47:00.893755Z","iopub.status.idle":"2026-02-07T06:47:13.283590Z","shell.execute_reply.started":"2026-02-07T06:47:00.893724Z","shell.execute_reply":"2026-02-07T06:47:13.282872Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: numpy 2.0.2\nUninstalling numpy-2.0.2:\n  Successfully uninstalled numpy-2.0.2\nCollecting numpy<2.0\n  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\ngoogle-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\njaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\njax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\ncudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\nbigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\npytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\npylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nRequirement already satisfied: scikit-surprise in /usr/local/lib/python3.12/dist-packages (1.1.4)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.5.3)\nRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-surprise) (1.15.3)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}